{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from rdquantum.hamiltonian import Rydberg_Cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hamiltonian\n",
    "H = Rydberg_Cz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('rdquantum/HamiltonianTrainer-v2023.04.22', Hamiltonian=Rydberg_Cz)\n",
    "observation, info = env.reset()\n",
    "# print(observation, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "algo = 'PPO'\n",
    "B = 30 # batch_size\n",
    "EPOCHS = 50\n",
    "eval_interval = 1\n",
    "lr = 1e-2 # 1e-2 for SGD\n",
    "policy_steps = 20\n",
    "\n",
    "log_prob_clip = 5\n",
    "grad_clip = 0.001\n",
    "importance_ratio_eps = 0.2\n",
    "value_loss_coeff = 0.5\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootdir = r'.'\n",
    "r_amp = H.r_amp\n",
    "r_gate_time = H.r_gate_time\n",
    "# print(r_amp, r_gate_time)\n",
    "\n",
    "\n",
    "# trainable variables\n",
    "actions = ['omega_p_amp', 'omega_r_amp', 'delta_p_amp', 'gate_time']\n",
    "mean = {s : tf.Variable(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32), name='mean_'+s) for s in actions}\n",
    "sigma = {s : tf.Variable(0.5, name='sigma_'+s) for s in actions}\n",
    "baseline = tf.Variable(0.0, name='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action = {\n",
    "    'omega_p_amp': [100/H.r_amp],\n",
    "    'omega_r_amp': [175/H.r_amp],\n",
    "    'delta_p_amp': [400/H.r_amp],\n",
    "    'gate_time': [1.0/H.r_gate_time]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_sampler(a, _type):\n",
    "    # print('a: ', a, '\\n')\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        a_np = {s : np.array(a[s][i] if _type=='collection' else [a[s]]) for s in actions}\n",
    "        # print('a_np: ', a_np, '\\n')\n",
    "        observation, reward, terminated, truncated, info = env.step(a_np)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return tf.cast(rewards, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(a, mean, sigma):\n",
    "    sigma_eps = 1e-5 # for mumerical stability\n",
    "    log_prob = 0.\n",
    "    for s in a.keys():\n",
    "        log_prob += - tf.math.log(tf.math.abs(sigma[s]) + sigma_eps) \\\n",
    "            - 0.5 * (a[s] - mean[s])**2 / (sigma[s]**2 + sigma_eps)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_sampler(actions, sample_batch = 1):\n",
    "    N = {s : tfp.distributions.TruncatedNormal(loc=mean[s], scale=sigma[s], low=0, high=1) for s in actions}\n",
    "    a = {s : N[s].sample(B) for s in actions}\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  tf.Tensor(\n",
      "[ 1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 2:  tf.Tensor(\n",
      "[ 1. -1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 3:  tf.Tensor(\n",
      "[-1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 4:  tf.Tensor(\n",
      "[ 1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 5:  tf.Tensor(\n",
      "[-1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.\n",
      "  1.  1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 6:  tf.Tensor(\n",
      "[-1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      " -1. -1.  1.  1.  1. -1.  1. -1. -1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 7:  tf.Tensor(\n",
      "[-1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 8:  tf.Tensor(\n",
      "[ 1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1.\n",
      " -1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 9:  tf.Tensor(\n",
      "[-1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 10:  tf.Tensor(\n",
      "[-1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 11:  tf.Tensor(\n",
      "[-1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 12:  tf.Tensor(\n",
      "[-1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 13:  tf.Tensor(\n",
      "[ 1.  1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "  1. -1.  1. -1. -1.  1.  1.  1.  1. -1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 14:  tf.Tensor(\n",
      "[ 1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 15:  tf.Tensor(\n",
      "[ 1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.\n",
      " -1. -1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 16:  tf.Tensor(\n",
      "[ 1. -1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 17:  tf.Tensor(\n",
      "[-1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.\n",
      " -1. -1. -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 18:  tf.Tensor(\n",
      "[ 1.  1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 19:  tf.Tensor(\n",
      "[-1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 20:  tf.Tensor(\n",
      "[ 1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 21:  tf.Tensor(\n",
      "[ 1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 22:  tf.Tensor(\n",
      "[ 1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 23:  tf.Tensor(\n",
      "[ 1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 24:  tf.Tensor(\n",
      "[-1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 25:  tf.Tensor(\n",
      "[-1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 26:  tf.Tensor(\n",
      "[-1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 27:  tf.Tensor(\n",
      "[-1.  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 28:  tf.Tensor(\n",
      "[ 1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1.\n",
      " -1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 29:  tf.Tensor(\n",
      "[ 1. -1. -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1.\n",
      "  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 30:  tf.Tensor(\n",
      "[-1.  1.  1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.\n",
      " -1.  1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 31:  tf.Tensor(\n",
      "[-1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      "  1. -1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 32:  tf.Tensor(\n",
      "[-1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1. -1.\n",
      "  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 33:  tf.Tensor(\n",
      "[-1.  1. -1. -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.\n",
      "  1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 34:  tf.Tensor(\n",
      "[-1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 35:  tf.Tensor(\n",
      "[ 1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1.  1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 36:  tf.Tensor(\n",
      "[-1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 37:  tf.Tensor(\n",
      "[ 1.  1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 38:  tf.Tensor(\n",
      "[-1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 39:  tf.Tensor(\n",
      "[ 1. -1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 40:  tf.Tensor(\n",
      "[-1.  1.  1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.\n",
      "  1. -1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 41:  tf.Tensor(\n",
      "[-1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 42:  tf.Tensor(\n",
      "[ 1. -1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1. -1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 43:  tf.Tensor(\n",
      "[ 1. -1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 44:  tf.Tensor(\n",
      "[ 1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 45:  tf.Tensor(\n",
      "[ 1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 46:  tf.Tensor(\n",
      "[ 1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.\n",
      " -1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 47:  tf.Tensor(\n",
      "[-1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 48:  tf.Tensor(\n",
      "[-1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.], shape=(30,), dtype=float32)\n",
      "Epoch 49:  tf.Tensor(\n",
      "[-1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.\n",
      "  1.  1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1.], shape=(30,), dtype=float32)\n",
      "Epoch 50:  tf.Tensor(\n",
      "[ 1.  1. -1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1.  1.  1. -1.  1.\n",
      " -1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.], shape=(30,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_samples = 0\n",
    "train_rewards = []\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "\n",
    "    train_samples += B\n",
    "    # print('train_samples:', train_samples)\n",
    "\n",
    "    # sample a batch of actions from Gaussian policy\n",
    "    a = action_sampler(actions, sample_batch=B)\n",
    "\n",
    "    # collect those rewards\n",
    "    R =  reward_sampler(a, 'collection')\n",
    "    # print('rewards: ', R)\n",
    "\n",
    "    # log prob according to old policy (required for importance ratio)\n",
    "    if epoch == 1: mean_old, sigma_old = mean, sigma\n",
    "    log_prob_old = compute_log_prob(a, mean_old, sigma_old)\n",
    "    log_prob_old = tf.clip_by_value(log_prob_old, -log_prob_clip, log_prob_clip)\n",
    "    mean_old = tf.nest.map_structure(tf.identity, mean)\n",
    "    sigma_old = tf.nest.map_structure(tf.identity, sigma)\n",
    "    \n",
    "    # calculate policy loss and do several gradient updates\n",
    "    for i in range(policy_steps):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # log prob according to the current policy\n",
    "            log_prob = compute_log_prob(a, mean, sigma)\n",
    "            log_prob = tf.clip_by_value(log_prob, -log_prob_clip, log_prob_clip)\n",
    "\n",
    "            A = R - baseline # action advantages\n",
    "\n",
    "            if algo == 'REINFORCE':\n",
    "                policy_loss_batch = - A * log_prob\n",
    "\n",
    "            if algo == 'PPO':\n",
    "                importance_ratio = tf.math.exp(log_prob - log_prob_old)\n",
    "                importance_ratio_clip = tf.clip_by_value(importance_ratio, \n",
    "                            1-importance_ratio_eps, 1+importance_ratio_eps)\n",
    "                policy_loss_batch = -tf.minimum(importance_ratio*A, importance_ratio_clip*A)\n",
    "\n",
    "            policy_loss = tf.reduce_mean(policy_loss_batch) # reduce over batch\n",
    "            value_loss = tf.reduce_mean(A**2)\n",
    "            loss = policy_loss + value_loss_coeff * value_loss\n",
    "            # print(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, tape.watched_variables())\n",
    "            grads = tf.clip_by_value(grads, -grad_clip, grad_clip)\n",
    "            optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            \n",
    "    print('Epoch %d: ' %(epoch), R)\n",
    "    train_rewards.append(np.array(R))\n",
    "    # log['train_actions'].append(np.array(a['theta']))\n",
    "    # log['train_epochs'].append(epoch)\n",
    "    # log['train_samples'].append(train_samples)\n",
    "    # if epoch % eval_interval == 0: evaluation(epoch, log)\n",
    "\n",
    "run_params = dict(B = B, EPOCHS = EPOCHS, eval_interval = eval_interval,\n",
    "        lr = lr, policy_steps = policy_steps, log_prob_clip = log_prob_clip,\n",
    "        grad_clip = grad_clip, importance_ratio_eps = importance_ratio_eps,\n",
    "        value_loss_coeff = value_loss_coeff)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30,), dtype=float32, numpy=\n",
       "array([ 1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
       "       -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "        1.,  1., -1., -1.], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1707,
     "status": "ok",
     "timestamp": 1681368543198,
     "user": {
      "displayName": "張子軒",
      "userId": "11622394988518650577"
     },
     "user_tz": -480
    },
    "id": "IXTctuZi0YqU",
    "outputId": "6de62e4a-2c7a-49b5-e637-367e0a0aa43b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = np.array(train_rewards)\n",
    "plt.matshow(np.reshape(result, (EPOCHS, B)).transpose())\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Episode\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEn6t9e6XqHq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMePkOqbTVxOTgKwEfY0Fg1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
